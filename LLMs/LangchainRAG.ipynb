{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Langchain RAG Course\n",
    "\n",
    "source: `https://blog.futuresmart.ai/langchain-rag-from-basics-to-production-ready-rag-chatbot#heading-the-process-typically-involves`\n",
    "\n",
    "## 1. Introduction\n",
    "### 1.1 What is Retrieval Augmented Generation (RAG)?\n",
    "\n",
    "RAG is a technique that enhances language models by combining them with a retrieval system. It allows the model to access and utilize external knowledge when generating responses.\n",
    "\n",
    "#### The process typically involves:\n",
    "* Indexing a large corpus of documents\n",
    "* Retrieving relevant information based on the input query\n",
    "* Using the retrieved information to augment the prompt sent to the language model\n",
    "\n",
    "### 1.2 What is Langchain?\n",
    "Langchain is a framework for developing applications powered by language models. It provides a set of tools and abstractions that make it easier to build complex AI applications. Key features include:\n",
    "\n",
    "* Modular components for common LLM tasks\n",
    "* Built-in support for various LLM providers\n",
    "* Tools for document loading, text splitting, and vector storage\n",
    "* Abstractions for building conversational agents and question-answering systems\n",
    "\n",
    "## 2. LangChain Components and Expression Language (LCEL)\n",
    "LangChain Expression Language (LCEL) is a key feature that makes working with LangChain components flexible and powerful. Let's explore how LCEL is used with various components:\n",
    "\n",
    "### 2.1. Large Language Model (LLM)\n",
    "LCEL allows direct invocation of the LLM:"
   ],
   "id": "ccdb126c1f72b228"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from openai import api_key\n",
    "!pip install -qU \"langchain[google-genai]\""
   ],
   "id": "6476fa2d57256d53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install dotenv",
   "id": "3d9433dc49c6667b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "gemini_key = os.getenv('GOOGLE_API_KEY')"
   ],
   "id": "6242261a8d2e4e2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\", api_key = gemini_key)"
   ],
   "id": "ca17f849f7c60b8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model.invoke('How are you?')",
   "id": "7ae4649027d00bb6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2 Output Parsers\n",
    "LCEL lets us chain the output parser directly to the LLM:"
   ],
   "id": "8cb113a925f7b29b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain = model | output_parser\n",
    "llm_response = chain.invoke(\"Tell me a joke\")\n",
    "print(llm_response)"
   ],
   "id": "8c303cc16cbd714d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.3 Structured Output\n",
    "LCEL allows us to create structured output chains:"
   ],
   "id": "5e5e5d97b6678ada"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field"
   ],
   "id": "9921a31d6c96c483"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MobileReview(BaseModel):\n",
    "        phone_model: str = Field(description=\"Name and model of the phone\")\n",
    "        rating: float = Field(description=\"Overall rating out of 5\")\n",
    "        pros: List[str] = Field(description=\"List of positive aspects\")\n",
    "        cons: List[str] = Field(description=\"List of negative aspects\")\n",
    "        summary: str = Field(description=\"Brief summary of the review\")\n",
    "\n",
    "review_text = \"\"\"\n",
    "    Just got my hands on the new Galaxy S21 and wow, this thing is slick! The screen is gorgeous,\n",
    "    colors pop like crazy. Camera's insane too, especially at night - my Insta game's never been\n",
    "    stronger. Battery life's solid, lasts me all day no problem.\n",
    "    Not gonna lie though, it's pretty pricey. And what's with ditching the charger? C'mon Samsung.\n",
    "    Also, still getting used to the new button layout, keep hitting Bixby by mistake.\n",
    "    Overall, I'd say it's a solid 4 out of 5. Great phone, but a few annoying quirks keep it from\n",
    "    being perfect. If you're due for an upgrade, definitely worth checking out!\n",
    "    \"\"\"\n",
    "\n",
    "structured_llm = model.with_structured_output(MobileReview)\n",
    "output = structured_llm.invoke(review_text)\n",
    "print(output)\n",
    "print(output.pros)"
   ],
   "id": "e5f54900618d61c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.4 Prompt Templates\n",
    "LCEL shines when working with prompt templates, allowing easy chaining:"
   ],
   "id": "3aa0989fbb1e4109"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "chain = prompt | model | output_parser\n",
    "result = chain.invoke({\"topic\": \"programming\"})"
   ],
   "id": "e4ff829d57eac678"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(result)",
   "id": "1121bdabbdc48842"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.5 LLM Messages\n",
    "\n",
    "LCEL allows flexible message composition:"
   ],
   "id": "ac4764df25995d13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "        SystemMessage(content=\"You are a helpful assistant that tells jokes.\"),\n",
    "        HumanMessage(content=\"Tell me about programming\")\n",
    "    ]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "print(response)"
   ],
   "id": "dd703b82795f524f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "template = ChatPromptTemplate([\n",
    "        (\"system\", \"You are a helpful assistant that tells jokes.\"),\n",
    "        (\"human\", \"Tell me about {topic}\")\n",
    "    ])\n",
    "chain = template | model\n",
    "template = ChatPromptTemplate([\n",
    "        (\"system\", \"You are a helpful assistant that tells jokes.\"),\n",
    "        (\"human\", \"Tell me about {topic}\")\n",
    "    ])\n",
    "chain = template | model\n",
    "response = chain.invoke({\"topic\": \"programming\"})\n",
    "print(response)"
   ],
   "id": "de62e62fe0438841"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Document Processing for RAG Systems\n",
    "After setting up our LangChain components, the next crucial step in building a RAG system is processing our documents. This involves loading the documents and splitting them into manageable chunks.\n",
    "\n",
    "### 3.1 Loading Documents\n",
    "We start by loading documents from various file types:"
   ],
   "id": "8d7556bd1db5d360"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "import os"
   ],
   "id": "fab88d1db1aad3e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_documents(folder_path: str) -> List[Document]:\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if filename.endswith('.pdf'):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif filename.endswith('.docx'):\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file type: {filename}\")\n",
    "            continue\n",
    "        documents.extend(loader.load())\n",
    "    return documents"
   ],
   "id": "c297299cb77e0dee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "folder_path = \"./examples\"\n",
    "documents = load_documents(folder_path=folder_path)"
   ],
   "id": "d4b2f15b34ae8e11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(f\"Loaded {len(documents)} documents from the folder.\")",
   "id": "1cf9f5d968a133be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.2 Splitting documents",
   "id": "babcb528562c90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"Split the documents into {len(splits)} chunks.\")"
   ],
   "id": "cadb8eee248de7e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(documents[0])",
   "id": "7020b863ab785932"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(splits[1])\n",
   "id": "96ea3056daa4a05f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(splits[0].metadata)",
   "id": "7c167b671b8e1094"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This metadata helps us keep track of where each piece of information came from, which can be crucial when the AI is using this information to answer questions.\n",
    "\n",
    "By processing our documents in this way, we're preparing the groundwork for our RAG system to efficiently retrieve relevant information when answering queries."
   ],
   "id": "367022bd61322b44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Creating Embeddings for RAG Systems\n",
    "\n",
    "After processing our documents, the next crucial step is to create embeddings. Embeddings are vector representations of our text chunks that allow for efficient similarity search, which is key to the retrieval part of our RAG system.\n",
    "\n",
    "### 4.1 Using OpenAI HuggingFaceEmbeddings\n"
   ],
   "id": "35c4b663eacc411e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T17:18:52.869454Z",
     "start_time": "2025-07-28T17:18:43.166123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "document_embeddings = embeddings.embed_documents([split.page_content for split in splits])\n",
    "\n",
    "print(f\"Created embeddings for {len(document_embeddings)} document chunks.\")"
   ],
   "id": "959c5f7b0866329f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lw/c1c6f3hx0_32q1532fprd37c0000gn/T/ipykernel_19598/90460912.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "/Users/octamarina/miniconda3/envs/Langchain/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/octamarina/miniconda3/envs/Langchain/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings for 6 document chunks.\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.2 Using SentenceTransformer",
   "id": "6b9f057c5e1b467c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T17:19:17.509835Z",
     "start_time": "2025-07-28T17:19:15.109946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "document_embeddings = embedding_function.embed_documents([split.page_content for split in splits])\n",
    "print(document_embeddings[0][:5])"
   ],
   "id": "485f37a0e2547ebb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0812104195356369, -0.05342952534556389, -0.044474635273218155, 0.035381168127059937, -0.053680337965488434]\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This method uses a pre-trained SentenceTransformer model to create embeddings locally, which can be faster and doesn't require API calls.\n",
    "\n",
    "The embeddings we've created are dense vector representations of our text chunks. Each vector typically has hundreds of dimensions (though we're only printing the first 5 here). These vectors capture semantic meaning, allowing us to find similar chunks of text by comparing their embeddings.\n",
    "\n",
    "In our RAG system, these embeddings will be crucial for quickly finding relevant information when answering queries. When a user asks a question, we'll create an embedding for that question and then find the most similar document chunks by comparing embeddings. This allows us to retrieve the most relevant information from our document collection efficiently.\n",
    "\n",
    "The next step will be to store these embeddings in a vector store, which will allow for fast similarity search during the retrieval phase of our RAG system."
   ],
   "id": "1c6dce9d03f4799"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Setting Up the Vector Store for RAG Systems\n",
    "\n",
    "Now that we have our document embeddings, we need a way to store and efficiently search through them. This is where a vector store comes in. We'll use Chroma, a popular vector store that integrates well with LangChain."
   ],
   "id": "e603de13b8c89dd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created and persisted to './chroma_db'\n"
     ]
    }
   ],
   "execution_count": 83,
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "collection_name = \"my_collection\"\n",
    "vectorstore = Chroma.from_documents(\n",
    "    collection_name=collection_name,\n",
    "    documents=splits,\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "print(\"Vector store created and persisted to './chroma_db'\")\n"
   ],
   "id": "16ace52984f6bed6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code creates a Chroma vector store from our document splits. It uses the embedding function we defined earlier to create embeddings for each document chunk. The vector store is then persisted to disk, allowing us to reuse it in future sessions without recomputing the embeddings.",
   "id": "1cae8ffabdcd9f2c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.1 Performing Similarity Search\n",
    "Now that our vector store is set up, we can perform similarity searches. This is a key component of the retrieval process in our RAG system:"
   ],
   "id": "c4c2c3cd84417499"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T17:28:38.087981Z",
     "start_time": "2025-07-28T17:28:37.668744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"When did Octavian Marina work?\"\n",
    "search_results = vectorstore.similarity_search(query, k=2)\n",
    "print(f\"\\nTop 2 most relevant chunks for the query: '{query}'\\n\")\n",
    "for i, result in enumerate(search_results, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"Source: {result.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"Content: {result.page_content}\")\n",
    "    print()"
   ],
   "id": "e1dba30059c67cfd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 2 most relevant chunks for the query: 'When did Octavian Marina work?'\n",
      "\n",
      "Result 1:\n",
      "Source: ./examples/Resume.pdf\n",
      "Content: 0740853147\n",
      "Cluj-Napoca, Romania\n",
      "octamarina@gmail.com\n",
      "Octavian Marina\n",
      "Data Scientist\n",
      "octavianmarina.com\n",
      "github.com/OctaMarina\n",
      "linkedin.com/egorhowell\n",
      "TECHNICAL SKILLS\n",
      "Languages Python, SQL, Java, JavaScript\n",
      "Tech Stack Git, Bash/Zsh, Snowflake, AWS, Jupyter Notebook (Anaconda), LangChain, Flask, HuggingFace\n",
      "EXPERIENCE\n",
      "Software Engineer March 2023 — Present\n",
      "Info World Cluj-Napoca, Romania\n",
      "\n",
      "Result 2:\n",
      "Source: ./examples/Resume.pdf\n",
      "Content: EXPERIENCE\n",
      "Software Engineer March 2023 — Present\n",
      "Info World Cluj-Napoca, Romania\n",
      "• Contributed to DICOM-compliant medical imaging software in the PACS suite, enabling efficient storage, retrieval,\n",
      "and visualization of radiological data.\n",
      "• Deployed an abdominal organ segmentation model for CT imaging within the web-based PACS platform, directly\n",
      "supporting the successful outcome of a competitive bidding process.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This similarity search finds the most relevant document chunks based on our query. The vector store compares the embedding of our query with the embeddings of all document chunks, returning the most similar ones.",
   "id": "204fecf9caf5059f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.2 Creating a Retriever\n",
    "We can also create a retriever from our vector store, which will be useful when we build our full RAG chain:"
   ],
   "id": "448f3d62f053e564"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T17:32:17.412888Z",
     "start_time": "2025-07-28T17:32:17.248559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "retriever_results = retriever.invoke(\"When did Octavian start working?\")\n",
    "print(retriever_results)"
   ],
   "id": "816256b6bb8123a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='8e5590f5-8b77-4d78-a6fc-65ec613c4f30', metadata={'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'keywords': '', 'producer': 'pdfTeX-1.40.26', 'trapped': '/False', 'subject': '', 'author': '', 'total_pages': 1, 'moddate': '2025-07-27T19:16:52+00:00', 'creator': 'LaTeX with hyperref', 'title': '', 'page_label': '', 'page': 0, 'source': './examples/Resume.pdf', 'creationdate': '2025-07-27T19:16:52+00:00'}, page_content='0740853147\\nCluj-Napoca, Romania\\noctamarina@gmail.com\\nOctavian Marina\\nData Scientist\\noctavianmarina.com\\ngithub.com/OctaMarina\\nlinkedin.com/egorhowell\\nTECHNICAL SKILLS\\nLanguages Python, SQL, Java, JavaScript\\nTech Stack Git, Bash/Zsh, Snowflake, AWS, Jupyter Notebook (Anaconda), LangChain, Flask, HuggingFace\\nEXPERIENCE\\nSoftware Engineer March 2023 — Present\\nInfo World Cluj-Napoca, Romania'), Document(id='a2c73346-5709-46f4-bf60-bb5cedf2048e', metadata={'creator': 'LaTeX with hyperref', 'page': 0, 'subject': '', 'title': '', 'trapped': '/False', 'total_pages': 1, 'source': './examples/Resume.pdf', 'page_label': '', 'producer': 'pdfTeX-1.40.26', 'keywords': '', 'creationdate': '2025-07-27T19:16:52+00:00', 'moddate': '2025-07-27T19:16:52+00:00', 'author': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0'}, page_content='EXPERIENCE\\nSoftware Engineer March 2023 — Present\\nInfo World Cluj-Napoca, Romania\\n• Contributed to DICOM-compliant medical imaging software in the PACS suite, enabling efficient storage, retrieval,\\nand visualization of radiological data.\\n• Deployed an abdominal organ segmentation model for CT imaging within the web-based PACS platform, directly\\nsupporting the successful outcome of a competitive bidding process.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/octamarina/miniconda3/envs/Langchain/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Building the RAG Chain",
   "id": "8fedf3834d9cc4cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T17:33:55.011941Z",
     "start_time": "2025-07-28T17:33:54.982266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ],
   "id": "f91ffa9f34f8aa41",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T17:34:00.233170Z",
     "start_time": "2025-07-28T17:34:00.229759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "Answer: \"\"\""
   ],
   "id": "74f29e94ab881d80",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T17:34:05.968841Z",
     "start_time": "2025-07-28T17:34:05.965729Z"
    }
   },
   "cell_type": "code",
   "source": "prompt = ChatPromptTemplate.from_template(template)",
   "id": "a936fd473b57d907",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T17:34:14.090172Z",
     "start_time": "2025-07-28T17:34:14.087178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def docs2str(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ],
   "id": "601724fdbdd59039",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T17:34:40.353653Z",
     "start_time": "2025-07-28T17:34:40.350990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "id": "6860304c37105b36",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Using the RAG Chain",
   "id": "ffd9078fb92f1602"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T17:36:27.525514Z",
     "start_time": "2025-07-28T17:36:26.829543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = \"Where do Octavian work?\"\n",
    "response = rag_chain.invoke(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {response}\")"
   ],
   "id": "3db0d346e272ce3d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/octamarina/miniconda3/envs/Langchain/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Where do Octavian work?\n",
      "Answer: Info World\n"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Handling Follow-Up Questions\n",
    "\n",
    "To make our RAG system more conversational, we need to handle follow-up questions effectively. This involves creating a history-aware retriever that can understand context from previous interactions.\n",
    "\n",
    "### 7.1 Creating a History-Aware Retriever\n",
    "First, let's set up the components for our history-aware retriever:"
   ],
   "id": "f7c83607a38341f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T18:25:44.469492Z",
     "start_time": "2025-07-28T18:25:44.467057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ],
   "id": "ba3f61d85a3aec7",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T18:25:44.655331Z",
     "start_time": "2025-07-28T18:25:44.652849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "contextualize_q_system_prompt = \"\"\"\n",
    "Given a chat history and the latest user question\n",
    "which might reference context in the chat history,\n",
    "formulate a standalone question which can be understood\n",
    "without the chat history. Do NOT answer the question,\n",
    "just reformulate it if needed and otherwise return it as is.\n",
    "\"\"\""
   ],
   "id": "8f658b50c0f4ba98",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T18:25:44.893342Z",
     "start_time": "2025-07-28T18:25:44.891051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ],
   "id": "11b52ad0bb2ff316",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T18:26:16.085464Z",
     "start_time": "2025-07-28T18:26:15.465505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "contextualize_chain = contextualize_q_prompt | model | StrOutputParser()\n",
    "print(contextualize_chain.invoke({\"input\": \"Where is it headquartered?\", \"chat_history\": []}))"
   ],
   "id": "b01b3686f7bf3d75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where is the company in question headquartered?\n"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T18:21:46.390371Z",
     "start_time": "2025-07-28T18:21:46.378212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    model, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Use the following context to answer the user's question.\"),\n",
    "    (\"system\", \"Context: {context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(model, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n"
   ],
   "id": "72897aefcefb5c7",
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T18:22:49.440502Z",
     "start_time": "2025-07-28T18:22:47.493829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = []\n",
    "question1 = \"Where do Octavian work?\"\n",
    "answer1 = rag_chain.invoke({\"input\": question1, \"chat_history\": chat_history})['answer']\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question1),\n",
    "    AIMessage(content=answer1)\n",
    "])\n",
    "\n",
    "print(f\"Human: {question1}\")\n",
    "print(f\"AI: {answer1}\\n\")\n",
    "\n",
    "question2 = \"What skills does he use in work?\"\n",
    "answer2 = rag_chain.invoke({\"input\": question2, \"chat_history\": chat_history})['answer']\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question2),\n",
    "    AIMessage(content=answer2)\n",
    "])\n",
    "\n",
    "print(f\"Human: {question2}\")\n",
    "print(f\"AI: {answer2}\")\n"
   ],
   "id": "f321369ada8b1663",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/octamarina/miniconda3/envs/Langchain/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Where do Octavian work?\n",
      "AI: Octavian works as a Software Engineer at Info World in Cluj-Napoca, Romania.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/octamarina/miniconda3/envs/Langchain/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What skills does he use in work?\n",
      "AI: While the document doesn't explicitly state which skills Octavian uses at work, it lists the following technical skills:\n",
      "\n",
      "*   **Languages:** Python, SQL, Java, JavaScript\n",
      "*   **Tech Stack:** Git, Bash/Zsh, Snowflake, AWS, Jupyter Notebook (Anaconda), LangChain, Flask, HuggingFace\n"
     ]
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8. Building a Multi-User Chatbot with SQLite Storage\n",
    "To make our RAG system more practical for real-world applications, we'll create a multi-user chatbot that stores conversation history in an SQLite database. This allows for persistent storage and retrieval of chat history across sessions."
   ],
   "id": "685c7781d614b0d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 8.1 Setting Up the SQLite Database\n",
    "First, let's set up our SQLite database and create the necessary functions for logging:"
   ],
   "id": "e586fa1a8d8d61ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T18:28:33.393766Z",
     "start_time": "2025-07-28T18:28:33.391581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import uuid"
   ],
   "id": "86c2d0fa8e8e6e66",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T18:29:55.949132Z",
     "start_time": "2025-07-28T18:29:55.945396Z"
    }
   },
   "cell_type": "code",
   "source": "DB_NAME = \"rag_app.db\"",
   "id": "729ffe3ac20695fb",
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T18:30:02.338693Z",
     "start_time": "2025-07-28T18:30:02.336580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_db_connection():\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    conn.row_factory = sqlite3.Row\n",
    "    return conn"
   ],
   "id": "ef72d69248e3455c",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T18:30:08.089012Z",
     "start_time": "2025-07-28T18:30:08.086437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_application_logs():\n",
    "    conn = get_db_connection()\n",
    "    conn.execute('''CREATE TABLE IF NOT EXISTS application_logs\n",
    "    (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    session_id TEXT,\n",
    "    user_query TEXT,\n",
    "    gpt_response TEXT,\n",
    "    model TEXT,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')\n",
    "    conn.close()"
   ],
   "id": "95f4dcab278d3a49",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T18:30:19.739596Z",
     "start_time": "2025-07-28T18:30:19.737140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def insert_application_logs(session_id, user_query, gpt_response, model):\n",
    "    conn = get_db_connection()\n",
    "    conn.execute('INSERT INTO application_logs (session_id, user_query, gpt_response, model) VALUES (?, ?, ?, ?)',\n",
    "                 (session_id, user_query, gpt_response, model))\n",
    "    conn.commit()\n",
    "    conn.close()"
   ],
   "id": "5e488d360e924ea4",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T18:30:25.658491Z",
     "start_time": "2025-07-28T18:30:25.656274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_chat_history(session_id):\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('SELECT user_query, gpt_response FROM application_logs WHERE session_id = ? ORDER BY created_at', (session_id,))\n",
    "    messages = []\n",
    "    for row in cursor.fetchall():\n",
    "        messages.extend([\n",
    "            {\"role\": \"human\", \"content\": row['user_query']},\n",
    "            {\"role\": \"ai\", \"content\": row['gpt_response']}\n",
    "        ])\n",
    "    conn.close()\n",
    "    return messages"
   ],
   "id": "89ced367ef2426bc",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1c3b2acbe3fe3cb1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
